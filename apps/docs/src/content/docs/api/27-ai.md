---
title: AI Features API
description: AI-powered video analysis, transcription, and automation
---

# ü§ñ AI Features API

## Overview

The AI domain provides intelligent automation features including automatic transcription, scene detection, content analysis, smart search, and AI-assisted feedback. These features help teams work more efficiently with video content.

---

## üìå Quick Reference

### Post-Launch Endpoints
| Endpoint | Type | Auth | Purpose |
|----------|------|------|---------|
| `ai.transcribe` | Mutation | Yes | Generate video transcript |
| `ai.getTranscript` | Query | Yes | Get transcript for video |

### Growth Endpoints
| Endpoint | Type | Auth | Purpose | Priority |
|----------|------|------|---------|----------|
| `ai.detectScenes` | Mutation | Yes | Auto-detect scene changes | High |
| `ai.generateSummary` | Mutation | Yes | Summarize video content | High |
| `ai.search` | Query | Yes | Semantic video search | High |
| `ai.suggestTags` | Query | Yes | Auto-suggest tags | Medium |
| `ai.analyzeContent` | Mutation | Yes | Content analysis (objects, faces) | Medium |

### Scale Endpoints
| Endpoint | Type | Auth | Purpose | Priority |
|----------|------|------|---------|----------|
| `ai.generateChapters` | Mutation | Yes | Auto-create video chapters | Medium |
| `ai.translateTranscript` | Mutation | Yes | Translate to other languages | Medium |
| `ai.generateCaptions` | Mutation | Yes | Generate caption file | Medium |
| `ai.suggestEdits` | Query | Yes | AI-powered edit suggestions | Low |
| `ai.compareVersions` | Query | Yes | AI version diff analysis | Low |

---

## üì¶ Data Models

### Transcript

```typescript
interface Transcript {
  id: string;
  videoId: string;
  
  // Content
  language: string;                // ISO 639-1 code
  text: string;                    // Full transcript text
  segments: TranscriptSegment[];
  
  // Metadata
  confidence: number;              // 0-1 overall confidence
  duration: number;                // Video duration in seconds
  wordCount: number;
  
  // Processing
  status: AIJobStatus;
  provider: string;                // e.g., 'whisper', 'deepgram'
  
  // Timestamps
  createdAt: DateTime;
  updatedAt: DateTime;
}

interface TranscriptSegment {
  id: string;
  startTime: number;               // Seconds
  endTime: number;
  text: string;
  confidence: number;
  
  // Word-level timing (optional)
  words?: Array<{
    word: string;
    startTime: number;
    endTime: number;
    confidence: number;
  }>;
  
  // Speaker (if diarization enabled)
  speaker?: string;
}

type AIJobStatus = 'pending' | 'processing' | 'completed' | 'failed';
```

### SceneDetection

```typescript
interface SceneDetection {
  id: string;
  videoId: string;
  
  scenes: Scene[];
  
  status: AIJobStatus;
  createdAt: DateTime;
}

interface Scene {
  id: string;
  index: number;
  startTime: number;
  endTime: number;
  duration: number;
  
  // Auto-generated
  thumbnailUrl: string;
  description?: string;            // AI-generated description
  
  // Analysis
  confidence: number;
  transitionType?: 'cut' | 'fade' | 'dissolve' | 'wipe';
}
```

### ContentAnalysis

```typescript
interface ContentAnalysis {
  id: string;
  videoId: string;
  
  // Detected elements
  objects: Array<{
    label: string;
    confidence: number;
    timestamps: Array<{ start: number; end: number }>;
    boundingBoxes?: Array<{ time: number; x: number; y: number; w: number; h: number }>;
  }>;
  
  faces: Array<{
    id: string;
    name?: string;                 // If recognized/labeled
    thumbnailUrl: string;
    appearances: Array<{ start: number; end: number }>;
  }>;
  
  text: Array<{                    // On-screen text (OCR)
    content: string;
    timestamp: number;
    boundingBox: { x: number; y: number; w: number; h: number };
  }>;
  
  // Overall
  topics: string[];
  sentiment: 'positive' | 'neutral' | 'negative';
  
  status: AIJobStatus;
  createdAt: DateTime;
}
```

### Drizzle Schema

```typescript
import { sqliteTable, text, integer, index } from "drizzle-orm/sqlite-core";
import { sql } from "drizzle-orm";

export const transcript = sqliteTable(
  "transcript",
  {
    id: text("id").primaryKey(),
    videoId: text("video_id")
      .notNull()
      .references(() => video.id, { onDelete: "cascade" }),
    
    language: text("language").default("en").notNull(),
    text: text("text").notNull(),
    segments: text("segments", { mode: "json" })
      .$type<TranscriptSegment[]>()
      .notNull(),
    
    confidence: integer("confidence").notNull(),  // Stored as 0-100
    duration: integer("duration").notNull(),
    wordCount: integer("word_count").notNull(),
    
    status: text("status", { 
      enum: ["pending", "processing", "completed", "failed"] 
    }).default("pending").notNull(),
    provider: text("provider"),
    error: text("error"),
    
    createdAt: integer("created_at", { mode: "timestamp_ms" })
      .default(sql`(cast(unixepoch('subsecond') * 1000 as integer))`)
      .notNull(),
    updatedAt: integer("updated_at", { mode: "timestamp_ms" })
      .default(sql`(cast(unixepoch('subsecond') * 1000 as integer))`)
      .$onUpdate(() => new Date())
      .notNull(),
  },
  (table) => [
    index("transcript_video_idx").on(table.videoId),
    index("transcript_status_idx").on(table.status),
  ]
);

export const sceneDetection = sqliteTable(
  "scene_detection",
  {
    id: text("id").primaryKey(),
    videoId: text("video_id")
      .notNull()
      .references(() => video.id, { onDelete: "cascade" }),
    
    scenes: text("scenes", { mode: "json" })
      .$type<Scene[]>()
      .notNull(),
    
    status: text("status", { 
      enum: ["pending", "processing", "completed", "failed"] 
    }).default("pending").notNull(),
    
    createdAt: integer("created_at", { mode: "timestamp_ms" })
      .default(sql`(cast(unixepoch('subsecond') * 1000 as integer))`)
      .notNull(),
  },
  (table) => [
    index("scene_detection_video_idx").on(table.videoId),
  ]
);

export const contentAnalysis = sqliteTable(
  "content_analysis",
  {
    id: text("id").primaryKey(),
    videoId: text("video_id")
      .notNull()
      .references(() => video.id, { onDelete: "cascade" }),
    
    objects: text("objects", { mode: "json" }).$type<unknown[]>(),
    faces: text("faces", { mode: "json" }).$type<unknown[]>(),
    textDetections: text("text_detections", { mode: "json" }).$type<unknown[]>(),
    topics: text("topics", { mode: "json" }).$type<string[]>(),
    sentiment: text("sentiment", { enum: ["positive", "neutral", "negative"] }),
    
    status: text("status", { 
      enum: ["pending", "processing", "completed", "failed"] 
    }).default("pending").notNull(),
    
    createdAt: integer("created_at", { mode: "timestamp_ms" })
      .default(sql`(cast(unixepoch('subsecond') * 1000 as integer))`)
      .notNull(),
  },
  (table) => [
    index("content_analysis_video_idx").on(table.videoId),
  ]
);
```

---

## üöÄ Post-Launch Endpoints

### 1. ai.transcribe

**Status:** üîÑ Post-Launch

**Purpose:** Generate automatic transcript for a video

**Type:** Mutation

**Auth Required:** Yes

**Permissions:** Must have edit access to video

**Input Schema:**

```typescript
{
  videoId: z.string(),
  language: z.string().length(2).default('en'),  // ISO 639-1
  options: z.object({
    speakerDiarization: z.boolean().default(false),
    wordTimestamps: z.boolean().default(true),
    punctuation: z.boolean().default(true),
  }).optional(),
}
```

**Response Schema:**

```typescript
{
  job: {
    id: string;
    status: AIJobStatus;
    estimatedTime: number;        // Seconds
  };
}
```

**Example Request:**

```typescript
const { job } = await trpc.ai.transcribe.mutate({
  videoId: "video_123",
  language: "en",
  options: {
    speakerDiarization: true,
    wordTimestamps: true,
  },
});

// Poll for completion
const transcript = await waitForTranscript(job.id);
```

**Example Response:**

```json
{
  "job": {
    "id": "job_transcribe_456",
    "status": "processing",
    "estimatedTime": 120
  }
}
```

**Error Codes:**

- `UNAUTHORIZED` - Not logged in
- `FORBIDDEN` - No edit access to video
- `NOT_FOUND` - Video not found
- `CONFLICT` - Transcription already in progress
- `BAD_REQUEST` - Unsupported language

**Business Rules:**

1. One transcription job per video at a time
2. Processing time ~1/4 of video duration
3. Existing transcript replaced on re-run
4. Supported languages: en, es, fr, de, it, pt, ja, ko, zh

---

### 2. ai.getTranscript

**Status:** üîÑ Post-Launch

**Purpose:** Get transcript for a video

**Type:** Query

**Auth Required:** Yes

**Permissions:** Must have view access to video

**Input Schema:**

```typescript
{
  videoId: z.string(),
  format: z.enum(['full', 'segments', 'srt', 'vtt']).default('full'),
}
```

**Response Schema:**

```typescript
{
  transcript: Transcript | null;
  // For srt/vtt formats
  content?: string;
}
```

**Example Response (full):**

```json
{
  "transcript": {
    "id": "tr_789",
    "videoId": "video_123",
    "language": "en",
    "text": "Welcome to our product demo. Today we'll be showing you...",
    "segments": [
      {
        "id": "seg_1",
        "startTime": 0.0,
        "endTime": 3.5,
        "text": "Welcome to our product demo.",
        "confidence": 0.95,
        "speaker": "Speaker 1"
      },
      {
        "id": "seg_2",
        "startTime": 3.5,
        "endTime": 7.2,
        "text": "Today we'll be showing you the new features.",
        "confidence": 0.92,
        "speaker": "Speaker 1"
      }
    ],
    "confidence": 0.94,
    "wordCount": 156,
    "status": "completed"
  }
}
```

**Example Response (vtt):**

```json
{
  "content": "WEBVTT\n\n00:00:00.000 --> 00:00:03.500\nWelcome to our product demo.\n\n00:00:03.500 --> 00:00:07.200\nToday we'll be showing you the new features.\n"
}
```

---

## üîÆ Growth Endpoints

### ai.detectScenes

**Priority:** High  
**Purpose:** Automatically detect scene changes  
**Complexity:** Medium

**Input:**

```typescript
{
  videoId: z.string(),
  sensitivity: z.enum(['low', 'medium', 'high']).default('medium'),
  generateDescriptions: z.boolean().default(false),
}
```

**Response:**

```typescript
{
  job: {
    id: string;
    status: AIJobStatus;
  };
}
```

**Result (when completed):**

```json
{
  "scenes": [
    {
      "id": "scene_1",
      "index": 0,
      "startTime": 0,
      "endTime": 15.5,
      "duration": 15.5,
      "thumbnailUrl": "https://...",
      "description": "Opening shot of office building exterior",
      "transitionType": "cut"
    },
    {
      "id": "scene_2",
      "index": 1,
      "startTime": 15.5,
      "endTime": 32.0,
      "duration": 16.5,
      "thumbnailUrl": "https://...",
      "description": "Interior meeting room with team discussion",
      "transitionType": "fade"
    }
  ]
}
```

---

### ai.generateSummary

**Priority:** High  
**Purpose:** Generate video summary  
**Complexity:** Medium

**Input:**

```typescript
{
  videoId: z.string(),
  length: z.enum(['brief', 'standard', 'detailed']).default('standard'),
  style: z.enum(['bullet_points', 'paragraph', 'executive']).default('paragraph'),
}
```

**Response:**

```typescript
{
  summary: {
    text: string;
    keyPoints: string[];
    topics: string[];
    duration: number;
  };
}
```

**Example Response:**

```json
{
  "summary": {
    "text": "This product demo video introduces the new collaboration features in Koko. The presenter walks through the updated comment system, real-time presence indicators, and approval workflows. Key highlights include the ability to @mention team members, inline video annotations, and streamlined review processes.",
    "keyPoints": [
      "New comment threading system",
      "Real-time collaboration features",
      "Simplified approval workflows",
      "Integration with external tools"
    ],
    "topics": ["product demo", "collaboration", "video review"],
    "duration": 180
  }
}
```

---

### ai.search

**Priority:** High  
**Purpose:** Semantic search across video content  
**Complexity:** Complex

**Input:**

```typescript
{
  query: z.string().min(1).max(500),
  projectId: z.string().optional(),
  filters: z.object({
    dateRange: z.object({
      from: z.date(),
      to: z.date(),
    }).optional(),
    uploadedBy: z.array(z.string()).optional(),
    tags: z.array(z.string()).optional(),
  }).optional(),
  limit: z.number().min(1).max(50).default(20),
}
```

**Response:**

```typescript
{
  results: Array<{
    videoId: string;
    video: Video;
    relevance: number;              // 0-1
    matches: Array<{
      type: 'transcript' | 'title' | 'description' | 'comment' | 'visual';
      timestamp?: number;
      text: string;
      highlight: string;            // With <mark> tags
    }>;
  }>;
  totalResults: number;
}
```

**Example Request:**

```typescript
const { results } = await trpc.ai.search.query({
  query: "product launch announcement",
  projectId: "project_123",
});
```

**Example Response:**

```json
{
  "results": [
    {
      "videoId": "video_456",
      "video": { "title": "Q1 Launch Video", "thumbnailUrl": "..." },
      "relevance": 0.92,
      "matches": [
        {
          "type": "transcript",
          "timestamp": 45.5,
          "text": "We're excited to announce the launch of our new product line",
          "highlight": "We're excited to <mark>announce the launch</mark> of our new <mark>product</mark> line"
        }
      ]
    }
  ],
  "totalResults": 5
}
```

---

### ai.suggestTags

**Priority:** Medium  
**Purpose:** Auto-suggest tags for video  
**Complexity:** Simple

**Input:**

```typescript
{
  videoId: z.string(),
  maxTags: z.number().min(1).max(20).default(10),
}
```

**Response:**

```typescript
{
  suggestions: Array<{
    tag: string;
    confidence: number;
    reason: string;
  }>;
}
```

---

### ai.analyzeContent

**Priority:** Medium  
**Purpose:** Analyze video for objects, faces, text  
**Complexity:** Complex

**Input:**

```typescript
{
  videoId: z.string(),
  features: z.object({
    objects: z.boolean().default(true),
    faces: z.boolean().default(true),
    text: z.boolean().default(true),
    topics: z.boolean().default(true),
    sentiment: z.boolean().default(false),
  }),
}
```

**Response:**

```typescript
{
  job: {
    id: string;
    status: AIJobStatus;
    estimatedTime: number;
  };
}
```

---

## üéØ Scale Endpoints

### ai.generateChapters

**Priority:** Medium  
**Purpose:** Auto-create video chapters from content  
**Complexity:** Medium

**Input:**

```typescript
{
  videoId: z.string(),
  maxChapters: z.number().min(2).max(20).default(10),
  minChapterLength: z.number().min(10).default(30),  // Seconds
}
```

**Response:**

```typescript
{
  chapters: Array<{
    title: string;
    startTime: number;
    endTime: number;
    summary?: string;
  }>;
}
```

---

### ai.translateTranscript

**Priority:** Medium  
**Purpose:** Translate transcript to other languages  
**Complexity:** Medium

**Input:**

```typescript
{
  videoId: z.string(),
  targetLanguage: z.string().length(2),  // ISO 639-1
}
```

**Response:**

```typescript
{
  transcript: Transcript;
}
```

---

### ai.generateCaptions

**Priority:** Medium  
**Purpose:** Generate caption file (SRT/VTT)  
**Complexity:** Simple

**Input:**

```typescript
{
  videoId: z.string(),
  format: z.enum(['srt', 'vtt']).default('vtt'),
  language: z.string().length(2).optional(),
  maxLineLength: z.number().min(20).max(80).default(42),
  maxLines: z.number().min(1).max(3).default(2),
}
```

**Response:**

```typescript
{
  content: string;
  fileName: string;
}
```

---

### ai.suggestEdits

**Priority:** Low  
**Purpose:** AI-powered edit suggestions  
**Complexity:** Complex

**Input:**

```typescript
{
  videoId: z.string(),
  focus: z.enum(['pacing', 'engagement', 'clarity', 'all']).default('all'),
}
```

**Response:**

```typescript
{
  suggestions: Array<{
    type: 'cut' | 'trim' | 'reorder' | 'audio' | 'visual';
    timestamp?: number;
    endTime?: number;
    description: string;
    reason: string;
    confidence: number;
    impact: 'low' | 'medium' | 'high';
  }>;
}
```

**Example Response:**

```json
{
  "suggestions": [
    {
      "type": "trim",
      "timestamp": 45.0,
      "endTime": 52.0,
      "description": "Consider trimming this pause",
      "reason": "7-second silence with no visual activity",
      "confidence": 0.85,
      "impact": "medium"
    },
    {
      "type": "audio",
      "timestamp": 120.5,
      "description": "Audio levels drop significantly",
      "reason": "Volume is 12dB below average in this section",
      "confidence": 0.92,
      "impact": "high"
    }
  ]
}
```

---

### ai.compareVersions

**Priority:** Low  
**Purpose:** AI analysis of version differences  
**Complexity:** Complex

**Input:**

```typescript
{
  videoId: z.string(),
  versionA: z.string(),
  versionB: z.string(),
}
```

**Response:**

```typescript
{
  comparison: {
    summary: string;
    differences: Array<{
      type: 'added' | 'removed' | 'changed';
      description: string;
      timestampA?: number;
      timestampB?: number;
      category: 'visual' | 'audio' | 'content' | 'timing';
    }>;
    statistics: {
      durationChange: number;      // Seconds
      sceneChanges: number;
      audioChanges: number;
      overallSimilarity: number;   // 0-1
    };
  };
}
```

---

## ‚öôÔ∏è AI Providers

### Transcription

| Provider | Features | Speed | Accuracy |
|----------|----------|-------|----------|
| OpenAI Whisper | Multi-language, word timing | Fast | High |
| Deepgram | Real-time, diarization | Very Fast | High |
| AssemblyAI | Summarization, topics | Medium | Very High |

### Vision Analysis

| Provider | Features |
|----------|----------|
| OpenAI GPT-4V | Scene description, content understanding |
| Google Cloud Vision | Object detection, OCR, face detection |
| AWS Rekognition | Face recognition, content moderation |

### Semantic Search

| Provider | Features |
|----------|----------|
| OpenAI Embeddings | Text similarity, semantic search |
| Pinecone | Vector database, fast retrieval |

---

## ‚ö° Performance & Quotas

### Processing Times

| Feature | Estimated Time |
|---------|----------------|
| Transcription | ~25% of video duration |
| Scene Detection | ~10% of video duration |
| Content Analysis | ~50% of video duration |
| Summary | ~10 seconds |
| Search | < 500ms |

### Quotas (per month)

| Tier | Transcription | Analysis | Search |
|------|---------------|----------|--------|
| Free | 60 min | 30 min | 100 queries |
| Pro | 600 min | 300 min | 1000 queries |
| Enterprise | Unlimited | Unlimited | Unlimited |

---

## üß™ Testing Scenarios

### Transcription Testing
- [ ] Transcribe English video
- [ ] Transcribe with speaker diarization
- [ ] Get transcript as VTT
- [ ] Handle failed transcription
- [ ] Re-transcribe existing video

### Scene Detection Testing
- [ ] Detect scenes with cuts
- [ ] Detect scenes with fades
- [ ] Generate scene descriptions
- [ ] Low/high sensitivity settings

### Search Testing
- [ ] Search by spoken content
- [ ] Search by visual content
- [ ] Filter by project/date
- [ ] Relevance ranking

---

## üìö Related Documentation

- [Videos API](./04-videos) - Video management
- [Search API](./16-search) - General search
- [Annotations API](./06-annotations) - Manual annotations

---

## üîó External Resources

- [OpenAI Whisper](https://openai.com/research/whisper)
- [Deepgram API](https://developers.deepgram.com/)
- [FFmpeg Scene Detection](https://ffmpeg.org/ffmpeg-filters.html#select_002c-aselect)
- [WebVTT Format](https://developer.mozilla.org/en-US/docs/Web/API/WebVTT_API)
